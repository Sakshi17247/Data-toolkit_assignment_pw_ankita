{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsQAV+Ab4LOuPkJU2+enWa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakshi17247/Data-toolkit_assignment_pw_ankita/blob/main/Copy_of_logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                                                 Theoretical\n",
        "                                                                 "
      ],
      "metadata": {
        "id": "WcoafSf8NxUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Ans = Logistic Regression predicts binary outcomes using a non-linear relationship, whereas Linear Regression predicts continuous outcomes using a linear relationship, differing in output type and estimation method."
      ],
      "metadata": {
        "id": "iMlTDgSGN4Bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "Ans = The mathematical equation for Logistic Regression is:\n",
        "\n",
        "p = 1 / (1 + e^(-z))\n",
        "\n",
        "where:\n",
        "\n",
        "p = probability of the positive class (0 ≤ p ≤ 1)\n",
        "e = base of the natural logarithm (approximately 2.718)\n",
        "z = linear combination of input features (w0 + w1_x1 + w2_x2 + … + wn*xn)\n",
        "\n",
        "This equation is also known as the Sigmoid function or Logit function."
      ],
      "metadata": {
        "id": "uBxpEvCVOh0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "Ans = We use the Sigmoid function in Logistic Regression for several reasons:\n",
        "\n",
        "1. Output Probability: The Sigmoid function maps any real-valued number to a value between 0 and 1, which is ideal for representing probabilities.\n",
        "\n",
        "2. Binary Classification: Logistic Regression is primarily used for binary classification problems, and the Sigmoid function provides a smooth, continuous transition between the two classes.\n",
        "\n",
        "3. Differentiability: The Sigmoid function is differentiable, which is necessary for optimizing the model using gradient-based methods.\n",
        "\n",
        "4. Interpretability: The Sigmoid function provides an interpretable output, where the probability of the positive class can be easily understood.\n",
        "\n",
        "5. Mathematical Convenience: The Sigmoid function has a convenient mathematical form that simplifies the calculation of probabilities and likelihoods."
      ],
      "metadata": {
        "id": "CgUAhdL9OzbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)  What is the cost function of Logistic Regression?\n",
        "\n",
        "Ans = The cost function of Logistic Regression is called the Log Loss or Cross-Entropy Loss function. It is defined as:\n",
        "\n",
        "J(θ) = - (1/m) * ∑[y(i) * log(hθ(x(i))) + (1-y(i)) * log(1-hθ(x(i)))]\n",
        "\n",
        "where:\n",
        "\n",
        "- J(θ) is the cost function\n",
        "- θ is the model's parameters\n",
        "- m is the number of training examples\n",
        "- y(i) is the true label of the i-th example\n",
        "- hθ(x(i)) is the predicted probability of the i-th example\n",
        "- x(i) is the i-th input feature vector\n",
        "\n",
        "The Log Loss function measures the difference between the predicted probabilities and the true labels, and it is minimized during the training process to optimize the model's parameters."
      ],
      "metadata": {
        "id": "fuST7G8kPZ_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)  What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Ans = Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function. This penalty term discourages large weights, encouraging the model to be simpler and generalize better.\n",
        "- Prevents overfitting\n",
        "- Simplifies the model\n",
        "- Encourages generalization\n",
        "- Types: L1 (Lasso), L2 (Ridge), Elastic Net"
      ],
      "metadata": {
        "id": "GOfWgqbKPqE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Explain the difference between Lasso, Ridge, and Elastic Net regression?\n",
        "\n",
        "Ans = Lasso, Ridge, and Elastic Net are regularization techniques used in linear regression to prevent overfitting:\n",
        "Lasso (L1)\n",
        "- Feature selection\n",
        "- Sets some coefficients to 0\n",
        "- Suitable for sparse models\n",
        "\n",
        "Ridge (L2)\n",
        "- Reduces coefficient magnitude\n",
        "- Suitable for correlated predictors\n",
        "\n",
        "Elastic Net\n",
        "- Combines L1 and L2\n",
        "- Balances feature selection and coefficient reduction"
      ],
      "metadata": {
        "id": "iNftyOqFP-Op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7)  When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Ans = Use Elastic Net instead of Lasso or Ridge in the following scenarios:\n",
        "\n",
        "1. Correlated features: When features are highly correlated, Elastic Net can handle this better than Lasso or Ridge alone.\n",
        "2. Mixed effect sizes: When some features have large effects and others have small effects, Elastic Net can balance these effects.\n",
        "3. High-dimensional data: In high-dimensional data, Elastic Net can provide better regularization and feature selection.\n",
        "4. When Lasso is too sparse: If Lasso selects too few features, Elastic Net can provide a more balanced solution.\n",
        "5. When Ridge is too dense: If Ridge includes too many features, Elastic Net can provide a more sparse solution."
      ],
      "metadata": {
        "id": "IIidbyocQrv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) C What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "Ans = The regularization parameter (λ) in Logistic Regression controls the strength of regularization. Here's its impact\n",
        "Increasing λ:\n",
        "- Reduces overfitting\n",
        "- Shrinks coefficients\n",
        "- Simplifies model\n",
        "\n",
        "Decreasing λ:\n",
        "- Increases overfitting\n",
        "- Increases coefficient magnitude\n",
        "- Increases model complexity"
      ],
      "metadata": {
        "id": "adj95ankQ_j_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) What are the key assumptions of Logistic Regression?\n",
        "\n",
        "Ans = The key assumptions of Logistic Regression are:\n",
        "\n",
        "1. Binary dependent variable: The response variable should be binary (0/1, yes/no, etc.).\n",
        "\n",
        "2. Independence of observations: Each observation should be independent of the others.\n",
        "\n",
        "3. Linearity in the logit: The log-odds (logit) should be linearly related to the predictors.\n",
        "\n",
        "4. No multicollinearity: Predictors should not be highly correlated with each other.\n",
        "\n",
        "5. Homoscedasticity: The variance of the residuals should be constant across all levels of the predictors.\n",
        "\n",
        "6. No significant outliers: There should be no significant outliers in the data.\n",
        "\n",
        "7. Correct functional form: The correct functional form of the relationship between predictors and response variable should be used.\n"
      ],
      "metadata": {
        "id": "t2Tfw47oRXoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10)  What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "Ans = Alternatives to Logistic Regression for classification tasks:\n",
        "\n",
        "1. Decision Trees\n",
        "2. Random Forests\n",
        "3. Support Vector Machines (SVMs)\n",
        "4. K-Nearest Neighbors (KNN)\n",
        "5. Neural Networks\n",
        "6. Gradient Boosting\n",
        "7. Naive Bayes\n",
        "\n",
        "These alternatives offer better performance, handling of non-linear relationships, and robustness to outliers."
      ],
      "metadata": {
        "id": "Ff9WhO3NRmlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11)  What are Classification Evaluation Metrics?\n",
        "\n",
        "Ans = Classification Evaluation Metrics are used to assess the performance of a classification model. Here are some common metrics:\n",
        "\n",
        "1. Accuracy: Proportion of correctly classified instances.\n",
        "\n",
        "2. Precision: Proportion of true positives among all positive predictions.\n",
        "\n",
        "3. Recall: Proportion of true positives among all actual positive instances.\n",
        "\n",
        "4. F1-score: Harmonic mean of precision and recall.\n",
        "\n",
        "5. Confusion Matrix: Table summarizing true positives, false positives, true negatives, and false negatives.\n",
        "\n",
        "6. ROC-AUC (Receiver Operating Characteristic-Area Under Curve): Measures model's ability to distinguish between classes.\n",
        "\n",
        "7. Classification Error: Proportion of misclassified instances.\n",
        "\n",
        "8. Specificity: Proportion of true negatives among all actual negative instances.\n",
        "\n",
        "9. Sensitivity: Same as recall.\n",
        "\n",
        "10. Kappa Statistic: Measures agreement between predicted and actual classes.\n",
        "\n",
        "These metrics help evaluate a classification model's performance, identify areas for improvement, and compare different models."
      ],
      "metadata": {
        "id": "O2cGTso8Skp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12) How does class imbalance affect Logistic Regression?\n",
        "\n",
        "Ans = Class imbalance affects Logistic Regression by:\n",
        "\n",
        "- Biased coefficients towards the majority class\n",
        "- Poor predictive performance on the minority class\n",
        "- Overfitting to the majority class\n",
        "- Difficulty in convergence\n",
        "\n",
        "To address class imbalance:\n",
        "\n",
        "- Oversample the minority class\n",
        "- Undersample the majority class\n",
        "- Use class weighting\n",
        "- Apply anomaly detection algorithms or ensemble methods"
      ],
      "metadata": {
        "id": "-auBVAVtSzSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13) What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Ans = Hyperparameter Tuning in Logistic Regression is the process of selecting the optimal hyperparameters for a Logistic Regression model to improve its performance. Hyperparameters are parameters that are set before training the model, such as\n",
        "- Selecting optimal hyperparameters to improve model performance\n",
        "- Hyperparameters: regularization strength, type, learning rate, max iterations\n",
        "- Techniques: Grid search, Random search, Bayesian optimization, Cross-validation\n",
        "- Goal: Find optimal hyperparameters for best performance on unseen data."
      ],
      "metadata": {
        "id": "a5DybJhBTAz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14)  What are different solvers in Logistic Regression? Which one should be used.?\n",
        "\n",
        "Ans = In Logistic Regression, solvers are algorithms used to optimize the model's parameters. Here are some common solvers\n",
        "1. Liblinear: Small to medium datasets\n",
        "2. Newton: Small to medium datasets\n",
        "3. lbfgs: Medium datasets\n",
        "4. sag: Large datasets\n",
        "5. saga: Large datasets, efficient and robust\n",
        "\n",
        "Choose a solver based on dataset size, computational resources, convergence speed, and regularization. Saga is a good default choice."
      ],
      "metadata": {
        "id": "695XbQggTRWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15) How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "Ans = Logistic Regression can be extended for multiclass classification using the following methods\n",
        "1. One-vs-Rest (OvR): Train separate binary models for each class.\n",
        "2. One-vs-All (OvA): Similar to OvR, but predict one class vs. all others.\n",
        "3. Multinomial Logistic Regression: Single model predicts probabilities for all classes.\n",
        "4. Softmax Regression: Specific type of Multinomial Logistic Regression using softmax activation."
      ],
      "metadata": {
        "id": "ntuYy-A6Tjji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16) What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "Ans = Here are the advantages and disadvantages of Logistic Regression:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "1. Easy to interpret: Logistic Regression provides interpretable results.\n",
        "2. Simple to implement: It's relatively simple to implement.\n",
        "3. Fast computation: Logistic Regression is computationally efficient.\n",
        "4. Handles categorical variables: It can handle categorical variables.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "1. Assumes linear relationship: Logistic Regression assumes a linear relationship.\n",
        "2. Not suitable for complex relationships: It's not suitable for modeling complex relationships.\n",
        "3. Sensitive to outliers: Logistic Regression can be affected by outliers.\n",
        "4. Limited to binary classification: It's typically used for binary classification problems."
      ],
      "metadata": {
        "id": "TGAN98PUT2jP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17) What are some use cases of Logistic Regression?\n",
        "\n",
        "Ans = Logistic Regression has numerous applications in various fields, including:\n",
        "\n",
        "1. Credit Risk Assessment: Predicting the likelihood of loan defaults based on credit score, income, and other factors.\n",
        "\n",
        "2. Medical Diagnosis: Identifying the probability of a patient having a particular disease based on symptoms, medical history, and test results.\n",
        "\n",
        "3. Customer Churn Prediction: Forecasting the likelihood of customers switching to a different service provider based on usage patterns, demographic data, and other factors.\n",
        "\n",
        "4. Spam Email Detection: Classifying emails as spam or non-spam based on content, sender, and recipient information.\n",
        "\n",
        "5. Marketing Response Prediction: Predicting the likelihood of a customer responding to a marketing campaign based on demographic data, purchase history, and other factors.\n",
        "\n",
        "6. Fraud Detection: Identifying potentially fraudulent transactions based on transaction patterns, user behavior, and other factors.\n",
        "\n",
        "7. Social Media Sentiment Analysis: Classifying social media posts as positive, negative, or neutral based on text content and other factors.\n",
        "\n",
        "8. Insurance Risk Assessment: Predicting the likelihood of insurance claims based on policyholder data, claim history, and other factors.\n",
        "\n",
        "9. Election Outcome Prediction: Forecasting the likelihood of election outcomes based on voter demographics, polling data, and other factors.\n",
        "\n",
        "10. Recommendation Systems: Predicting user preferences for products or services based on user behavior, demographic data, and other factors."
      ],
      "metadata": {
        "id": "rJcFjMBxUHIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18)  What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Ans = Softmax Regression and Logistic Regression are both supervised learning algorithms used for classification problems. The key differences between them are\n",
        "Differences:\n",
        "\n",
        "1. Number of classes: Logistic (2), Softmax (multiple)\n",
        "2. Output: Logistic (probability), Softmax (probability distribution)\n",
        "3. Activation function: Logistic (sigmoid), Softmax (softmax)\n",
        "4. Loss function: Logistic (binary cross-entropy), Softmax (categorical cross-entropy)\n",
        "\n",
        "Similarities:\n",
        "\n",
        "1. Linear model\n",
        "2. Supervised learning\n",
        "3. Optimization technique\n"
      ],
      "metadata": {
        "id": "NUBNXS_vUTaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19) How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "Ans = Choosing between One-vs-Rest (OvR) and Softmax for multiclass classification depends on the specific problem and dataset. Here are some factors to consider\n",
        "OvR:\n",
        "- Interpretability\n",
        "- Flexibility\n",
        "- Handles imbalanced datasets\n",
        "\n",
        "Softmax:\n",
        "- Efficiency\n",
        "- Multiclass classification\n",
        "- Probabilistic output\n",
        "\n",
        "Choose based on:\n",
        "\n",
        "- Number of classes\n",
        "- Dataset size\n",
        "- Class balance\n",
        "- Interpretability\n",
        "\n"
      ],
      "metadata": {
        "id": "bGz8IfXnUp8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20) How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Ans = In Logistic Regression, coefficients represent the change in the log-odds of the outcome variable for a one-unit change in the predictor variable, while holding all other predictor variables constant.\n",
        "1. Positive coefficient: Increase in predictor variable → Increase in log-odds of outcome variable.\n",
        "2. Negative coefficient: Increase in predictor variable → Decrease in log-odds of outcome variable.\n",
        "3. Odds ratio (e^coefficient): Change in odds of outcome variable for a one-unit change in predictor variable.\n",
        "\n",
        "Coefficients represent the change in log-odds while holding other variables constant."
      ],
      "metadata": {
        "id": "GthsMML7U_Az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0ohhj0P4VP7d"
      }
    }
  ]
}